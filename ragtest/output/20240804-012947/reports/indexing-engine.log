01:29:47,246 graphrag.config.read_dotenv INFO Loading pipeline .env file
01:29:47,248 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 56",
        "type": "openai_chat",
        "model": "gpt-3.5-turbo",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "./ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "output/${timestamp}/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/${timestamp}/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 56",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 56",
            "type": "openai_chat",
            "model": "gpt-3.5-turbo",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 56",
            "type": "openai_chat",
            "model": "gpt-3.5-turbo",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 56",
            "type": "openai_chat",
            "model": "gpt-3.5-turbo",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 56",
            "type": "openai_chat",
            "model": "gpt-3.5-turbo",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
01:29:47,249 graphrag.index.create_pipeline_config INFO skipping workflows 
01:29:47,259 graphrag.index.run INFO Running pipeline
01:29:47,259 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at ragtest/output/20240804-012947/artifacts
01:29:47,259 graphrag.index.input.load_input INFO loading input from root_dir=input
01:29:47,259 graphrag.index.input.load_input INFO using file storage for input
01:29:47,260 graphrag.index.storage.file_pipeline_storage INFO search ragtest/input for files matching .*\.txt$
01:29:47,260 graphrag.index.input.text INFO found text files from input, found [('grant_12.txt', {}), ('grant_2.txt', {}), ('grant_3.txt', {}), ('grant_13.txt', {}), ('grant_39.txt', {}), ('grant_11.txt', {}), ('grant_1.txt', {}), ('grant_10.txt', {}), ('grant_38.txt', {}), ('grant_14.txt', {}), ('grant_28.txt', {}), ('grant_4.txt', {}), ('grant_5.txt', {}), ('grant_29.txt', {}), ('grant_15.txt', {}), ('grant_17.txt', {}), ('grant_7.txt', {}), ('grant_6.txt', {}), ('grant_16.txt', {}), ('grant_42.txt', {}), ('grant_41.txt', {}), ('grant_40.txt', {}), ('grant_27.txt', {}), ('grant_33.txt', {}), ('grant_32.txt', {}), ('grant_26.txt', {}), ('grant_18.txt', {}), ('grant_30.txt', {}), ('grant_24.txt', {}), ('grant_8.txt', {}), ('grant_9.txt', {}), ('grant_25.txt', {}), ('grant_31.txt', {}), ('grant_19.txt', {}), ('grant_35.txt', {}), ('grant_21.txt', {}), ('grant_20.txt', {}), ('grant_34.txt', {}), ('grant_22.txt', {}), ('grant_36.txt', {}), ('grant_37.txt', {}), ('grant_23.txt', {})]
01:29:47,276 graphrag.index.input.text INFO Found 42 files, loading 42
01:29:47,278 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
01:29:47,278 graphrag.index.run INFO Final # of rows loaded: 42
01:29:47,345 graphrag.index.run INFO Running workflow: create_base_text_units...
01:29:47,345 graphrag.index.run INFO dependencies for create_base_text_units: []
01:29:47,347 datashaper.workflow.workflow INFO executing verb orderby
01:29:47,351 datashaper.workflow.workflow INFO executing verb zip
01:29:47,353 datashaper.workflow.workflow INFO executing verb aggregate_override
01:29:47,357 datashaper.workflow.workflow INFO executing verb chunk
01:29:47,485 datashaper.workflow.workflow INFO executing verb select
01:29:47,488 datashaper.workflow.workflow INFO executing verb unroll
01:29:47,492 datashaper.workflow.workflow INFO executing verb rename
01:29:47,494 datashaper.workflow.workflow INFO executing verb genid
01:29:47,499 datashaper.workflow.workflow INFO executing verb unzip
01:29:47,501 datashaper.workflow.workflow INFO executing verb copy
01:29:47,503 datashaper.workflow.workflow INFO executing verb filter
01:29:47,509 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:29:47,603 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
01:29:47,603 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
01:29:47,604 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
01:29:47,623 datashaper.workflow.workflow INFO executing verb entity_extract
01:29:47,627 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
01:29:47,634 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-3.5-turbo: TPM=0, RPM=0
01:29:47,634 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-3.5-turbo: 25
01:29:50,21 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:50,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.323999999964144. input_tokens=2349, output_tokens=187
01:29:50,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:50,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.727000000013504. input_tokens=2568, output_tokens=213
01:29:50,712 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:50,713 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.0289999999804422. input_tokens=2342, output_tokens=220
01:29:51,125 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:51,127 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:51,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.47099999996135. input_tokens=2895, output_tokens=272
01:29:51,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.4420000000391155. input_tokens=2378, output_tokens=273
01:29:51,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:51,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.087999999988824. input_tokens=2621, output_tokens=415
01:29:52,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:52,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.565999999991618. input_tokens=3134, output_tokens=432
01:29:52,302 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:52,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.622000000032131. input_tokens=3134, output_tokens=436
01:29:52,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:52,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.618000000016764. input_tokens=2554, output_tokens=390
01:29:52,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:52,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.6700000000419095. input_tokens=2480, output_tokens=447
01:29:52,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:52,378 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.701000000000931. input_tokens=2858, output_tokens=379
01:29:52,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:52,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.702999999979511. input_tokens=3133, output_tokens=331
01:29:52,697 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:52,701 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.0310000000172295. input_tokens=3134, output_tokens=442
01:29:52,775 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:52,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.098999999987427. input_tokens=2766, output_tokens=432
01:29:53,197 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:53,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.538000000000466. input_tokens=2537, output_tokens=461
01:29:53,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:53,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.583999999973457. input_tokens=2810, output_tokens=561
01:29:53,495 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:53,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.469000000040978. input_tokens=2827, output_tokens=383
01:29:53,571 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:53,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.900000000023283. input_tokens=3134, output_tokens=536
01:29:53,595 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:53,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.931999999971595. input_tokens=3134, output_tokens=590
01:29:54,500 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:54,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.798000000009779. input_tokens=3132, output_tokens=639
01:29:54,518 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:54,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.809000000008382. input_tokens=3028, output_tokens=401
01:29:54,806 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:54,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.421000000031199. input_tokens=2670, output_tokens=459
01:29:55,370 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:55,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.1120000000228174. input_tokens=3134, output_tokens=301
01:29:55,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:55,574 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:55,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.8319999999948777. input_tokens=3135, output_tokens=352
01:29:55,585 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.896999999997206. input_tokens=3133, output_tokens=576
01:29:55,754 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:55,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.103000000002794. input_tokens=2312, output_tokens=851
01:29:56,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:56,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.820000000006985. input_tokens=3133, output_tokens=358
01:29:56,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:56,415 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.1560000000172295. input_tokens=3134, output_tokens=299
01:29:56,457 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:56,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.054999999993015. input_tokens=2320, output_tokens=385
01:29:56,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:56,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.187999999965541. input_tokens=3134, output_tokens=506
01:29:56,676 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:56,680 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.548999999999069. input_tokens=2879, output_tokens=519
01:29:56,775 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:56,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.642000000050757. input_tokens=3134, output_tokens=613
01:29:56,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:56,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.15600000001723. input_tokens=2714, output_tokens=839
01:29:56,873 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:56,875 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.206999999994878. input_tokens=3133, output_tokens=445
01:29:57,30 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:57,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.65300000004936. input_tokens=3134, output_tokens=446
01:29:57,133 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:57,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3779999999678694. input_tokens=19, output_tokens=99
01:29:57,146 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:57,148 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.50099999998929. input_tokens=3134, output_tokens=493
01:29:57,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:57,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.0689999999594875. input_tokens=3134, output_tokens=385
01:29:57,436 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:57,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.738000000012107. input_tokens=3134, output_tokens=532
01:29:57,440 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:57,446 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.8730000000214204. input_tokens=2710, output_tokens=349
01:29:57,991 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:57,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4360000000451691. input_tokens=19, output_tokens=126
01:29:58,109 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.2999999999883585. input_tokens=3133, output_tokens=333
01:29:58,224 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.627999999967869. input_tokens=3134, output_tokens=510
01:29:58,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5769999999902211. input_tokens=19, output_tokens=178
01:29:58,303 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.8020000000251457. input_tokens=3134, output_tokens=326
01:29:58,334 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.875. input_tokens=19, output_tokens=148
01:29:58,362 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.9470000000437722. input_tokens=19, output_tokens=169
01:29:58,440 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.315999999991618. input_tokens=19, output_tokens=120
01:29:58,720 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,723 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8570000000181608. input_tokens=19, output_tokens=166
01:29:58,944 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8089999999501742. input_tokens=19, output_tokens=150
01:29:58,987 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:58,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.547000000020489. input_tokens=19, output_tokens=163
01:29:59,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:59,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.488000000012107. input_tokens=3134, output_tokens=639
01:29:59,288 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:59,290 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.789999999979045. input_tokens=2814, output_tokens=613
01:29:59,308 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:59,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.161000000021886. input_tokens=19, output_tokens=180
01:29:59,373 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:59,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.9280000000144355. input_tokens=19, output_tokens=188
01:29:59,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:59,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.0469999999622814. input_tokens=19, output_tokens=217
01:29:59,826 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:59,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.258000000030734. input_tokens=2639, output_tokens=363
01:29:59,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:29:59,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5370000000111759. input_tokens=19, output_tokens=152
01:30:00,82 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:00,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.772999999986496. input_tokens=2799, output_tokens=767
01:30:00,175 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:00,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.0640000000130385. input_tokens=19, output_tokens=209
01:30:00,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:00,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.0239999999757856. input_tokens=19, output_tokens=143
01:30:00,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:00,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3709999999846332. input_tokens=19, output_tokens=149
01:30:00,853 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:00,858 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.8260000000009313. input_tokens=19, output_tokens=378
01:30:00,923 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:00,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.2000000000116415. input_tokens=19, output_tokens=262
01:30:00,946 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:00,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.576000000000931. input_tokens=2808, output_tokens=434
01:30:01,83 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8150000000023283. input_tokens=19, output_tokens=162
01:30:01,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.878999999957159. input_tokens=19, output_tokens=344
01:30:01,267 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.99900000001071. input_tokens=19, output_tokens=436
01:30:01,380 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.798000000009779. input_tokens=3134, output_tokens=535
01:30:01,404 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.041999999957625. input_tokens=19, output_tokens=343
01:30:01,696 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,703 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.444000000017695. input_tokens=19, output_tokens=354
01:30:01,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,831 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.3060000000405125. input_tokens=3133, output_tokens=619
01:30:01,890 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.602000000013504. input_tokens=19, output_tokens=251
01:30:01,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.566999999980908. input_tokens=19, output_tokens=164
01:30:01,986 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:01,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.992999999958556. input_tokens=19, output_tokens=442
01:30:02,17 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:02,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8410000000149012. input_tokens=19, output_tokens=204
01:30:02,103 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:02,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:02,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.316999999980908. input_tokens=19, output_tokens=171
01:30:02,233 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.534999999974389. input_tokens=3134, output_tokens=1467
01:30:02,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:02,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.3130000000237487. input_tokens=19, output_tokens=193
01:30:02,649 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:02,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.3170000000391155. input_tokens=19, output_tokens=415
01:30:03,339 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:03,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.466000000014901. input_tokens=19, output_tokens=591
01:30:03,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:03,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.408999999985099. input_tokens=19, output_tokens=285
01:30:03,509 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:03,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.651000000012573. input_tokens=2107, output_tokens=222
01:30:03,738 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:03,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.752999999967869. input_tokens=19, output_tokens=380
01:30:04,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:04,142 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.194000000017695. input_tokens=3134, output_tokens=271
01:30:04,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:04,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.461999999999534. input_tokens=19, output_tokens=300
01:30:04,290 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:04,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.2149999999674037. input_tokens=3134, output_tokens=285
01:30:04,492 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:04,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.3860000000568107. input_tokens=3134, output_tokens=339
01:30:04,570 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:04,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.338999999978114. input_tokens=2430, output_tokens=204
01:30:04,893 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:04,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.212999999988824. input_tokens=3135, output_tokens=350
01:30:04,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:04,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.12300000002142. input_tokens=19, output_tokens=471
01:30:05,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:05,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.00099999998929. input_tokens=2431, output_tokens=349
01:30:05,430 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:05,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.4429999999701977. input_tokens=3133, output_tokens=322
01:30:05,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:05,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.6739999999990687. input_tokens=2979, output_tokens=344
01:30:05,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:05,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.2989999999990687. input_tokens=19, output_tokens=192
01:30:05,882 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:05,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.940999999991618. input_tokens=3134, output_tokens=449
01:30:05,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:05,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.650000000023283. input_tokens=2653, output_tokens=354
01:30:05,953 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:05,956 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.213000000047032. input_tokens=19, output_tokens=189
01:30:06,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:06,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.191999999980908. input_tokens=3134, output_tokens=438
01:30:06,212 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:06,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.054999999993015. input_tokens=3134, output_tokens=401
01:30:06,436 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:06,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.7849999999743886. input_tokens=2867, output_tokens=350
01:30:06,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:06,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.179999999993015. input_tokens=3134, output_tokens=616
01:30:06,777 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:06,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.2850000000325963. input_tokens=2929, output_tokens=328
01:30:06,860 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:06,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.160000000032596. input_tokens=2742, output_tokens=462
01:30:06,882 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:06,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.989999999990687. input_tokens=2791, output_tokens=479
01:30:07,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:07,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.989000000001397. input_tokens=3134, output_tokens=558
01:30:07,755 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:07,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.798000000009779. input_tokens=2063, output_tokens=257
01:30:07,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:07,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.3319999999948777. input_tokens=3116, output_tokens=287
01:30:07,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:07,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.3880000000353903. input_tokens=2016, output_tokens=413
01:30:08,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:08,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.010999999998603. input_tokens=2941, output_tokens=585
01:30:08,418 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:08,425 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.0090000000200234. input_tokens=2726, output_tokens=276
01:30:08,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:08,430 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.5350000000325963. input_tokens=3133, output_tokens=321
01:30:09,29 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:09,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.88900000002468. input_tokens=3134, output_tokens=484
01:30:09,261 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:09,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.962999999988824. input_tokens=2600, output_tokens=443
01:30:09,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:09,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.4280000000144355. input_tokens=2920, output_tokens=445
01:30:10,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:10,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.76500000001397. input_tokens=2438, output_tokens=525
01:30:10,591 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:10,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.325000000011642. input_tokens=2723, output_tokens=639
01:30:13,458 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:13,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.12300000002142. input_tokens=2763, output_tokens=550
01:30:13,487 datashaper.workflow.workflow INFO executing verb merge_graphs
01:30:13,529 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
01:30:13,697 graphrag.index.run INFO Running workflow: create_summarized_entities...
01:30:13,697 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
01:30:13,698 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
01:30:13,705 datashaper.workflow.workflow INFO executing verb summarize_descriptions
01:30:14,386 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.625. input_tokens=154, output_tokens=29
01:30:14,403 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6730000000097789. input_tokens=169, output_tokens=28
01:30:14,498 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7580000000307336. input_tokens=255, output_tokens=45
01:30:14,525 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7920000000158325. input_tokens=177, output_tokens=45
01:30:14,537 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,538 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7800000000279397. input_tokens=164, output_tokens=41
01:30:14,539 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7750000000232831. input_tokens=184, output_tokens=45
01:30:14,548 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7770000000018626. input_tokens=183, output_tokens=38
01:30:14,576 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8169999999809079. input_tokens=154, output_tokens=49
01:30:14,592 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8229999999748543. input_tokens=192, output_tokens=48
01:30:14,683 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,684 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9400000000023283. input_tokens=194, output_tokens=58
01:30:14,692 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9259999999776483. input_tokens=205, output_tokens=57
01:30:14,695 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9299999999930151. input_tokens=238, output_tokens=62
01:30:14,725 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9619999999995343. input_tokens=156, output_tokens=69
01:30:14,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.003000000026077. input_tokens=237, output_tokens=69
01:30:14,782 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.036000000021886. input_tokens=395, output_tokens=77
01:30:14,855 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,856 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1080000000074506. input_tokens=248, output_tokens=77
01:30:14,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.125. input_tokens=226, output_tokens=77
01:30:14,880 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.128000000026077. input_tokens=342, output_tokens=91
01:30:14,882 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,888 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1520000000018626. input_tokens=180, output_tokens=53
01:30:14,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:14,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2339999999967404. input_tokens=228, output_tokens=97
01:30:15,7 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2699999999604188. input_tokens=291, output_tokens=72
01:30:15,13 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2800000000279397. input_tokens=171, output_tokens=42
01:30:15,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2820000000065193. input_tokens=233, output_tokens=68
01:30:15,159 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7729999999864958. input_tokens=174, output_tokens=47
01:30:15,289 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7409999999799766. input_tokens=226, output_tokens=57
01:30:15,307 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,308 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5589999999501742. input_tokens=263, output_tokens=76
01:30:15,342 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6460000000079162. input_tokens=165, output_tokens=38
01:30:15,433 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8630000000121072. input_tokens=166, output_tokens=64
01:30:15,489 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,490 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8059999999823049. input_tokens=242, output_tokens=60
01:30:15,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0869999999995343. input_tokens=180, output_tokens=61
01:30:15,518 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.76400000002468. input_tokens=178, output_tokens=78
01:30:15,533 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.007999999972526. input_tokens=214, output_tokens=80
01:30:15,582 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0380000000004657. input_tokens=163, output_tokens=49
01:30:15,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0669999999809079. input_tokens=189, output_tokens=61
01:30:15,662 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9710000000195578. input_tokens=161, output_tokens=35
01:30:15,693 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,694 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8119999999762513. input_tokens=198, output_tokens=52
01:30:15,766 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0129999999771826. input_tokens=191, output_tokens=52
01:30:15,771 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,772 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2729999999864958. input_tokens=182, output_tokens=112
01:30:15,774 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9139999999897555. input_tokens=189, output_tokens=66
01:30:15,819 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9309999999823049. input_tokens=219, output_tokens=78
01:30:15,851 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2570000000414439. input_tokens=191, output_tokens=77
01:30:15,865 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,866 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1400000000139698. input_tokens=188, output_tokens=57
01:30:15,954 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9370000000344589. input_tokens=182, output_tokens=53
01:30:15,969 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:15,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9479999999748543. input_tokens=181, output_tokens=69
01:30:16,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1640000000479631. input_tokens=189, output_tokens=67
01:30:16,50 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.529999999969732. input_tokens=175, output_tokens=31
01:30:16,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8359999999520369. input_tokens=200, output_tokens=61
01:30:16,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8380000000470318. input_tokens=178, output_tokens=59
01:30:16,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1780000000144355. input_tokens=192, output_tokens=69
01:30:16,278 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9359999999869615. input_tokens=176, output_tokens=39
01:30:16,308 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3190000000176951. input_tokens=252, output_tokens=117
01:30:16,472 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,474 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9820000000181608. input_tokens=209, output_tokens=81
01:30:16,489 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7959999999729916. input_tokens=183, output_tokens=56
01:30:16,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8950000000186265. input_tokens=189, output_tokens=52
01:30:16,553 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.88900000002468. input_tokens=169, output_tokens=40
01:30:16,585 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8030000000144355. input_tokens=295, output_tokens=126
01:30:16,646 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6780000000144355. input_tokens=203, output_tokens=53
01:30:16,658 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0750000000116415. input_tokens=221, output_tokens=63
01:30:16,687 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.665999999968335. input_tokens=164, output_tokens=40
01:30:16,703 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2619999999878928. input_tokens=256, output_tokens=86
01:30:16,711 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.551000000035856. input_tokens=247, output_tokens=114
01:30:16,743 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5970000000088476. input_tokens=169, output_tokens=30
01:30:16,797 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.978000000002794. input_tokens=239, output_tokens=65
01:30:16,831 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,833 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0559999999823049. input_tokens=238, output_tokens=65
01:30:16,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.364000000001397. input_tokens=186, output_tokens=86
01:30:16,863 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0970000000088476. input_tokens=196, output_tokens=65
01:30:16,887 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,888 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8360000000102445. input_tokens=192, output_tokens=64
01:30:16,911 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1380000000353903. input_tokens=219, output_tokens=89
01:30:16,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.070000000006985. input_tokens=196, output_tokens=71
01:30:16,953 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:16,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.760999999998603. input_tokens=169, output_tokens=46
01:30:17,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5179999999818392. input_tokens=217, output_tokens=91
01:30:17,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,80 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6050000000395812. input_tokens=165, output_tokens=35
01:30:17,265 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7660000000032596. input_tokens=184, output_tokens=59
01:30:17,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6890000000130385. input_tokens=182, output_tokens=45
01:30:17,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3269999999902211. input_tokens=205, output_tokens=83
01:30:17,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1959999999962747. input_tokens=177, output_tokens=25
01:30:17,328 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4759999999660067. input_tokens=331, output_tokens=100
01:30:17,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8040000000037253. input_tokens=190, output_tokens=58
01:30:17,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7260000000242144. input_tokens=161, output_tokens=51
01:30:17,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2760000000125729. input_tokens=222, output_tokens=88
01:30:17,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2589999999618158. input_tokens=192, output_tokens=78
01:30:17,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9249999999883585. input_tokens=169, output_tokens=42
01:30:17,603 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7449999999953434. input_tokens=170, output_tokens=24
01:30:17,609 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,610 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9619999999995343. input_tokens=213, output_tokens=73
01:30:17,663 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7260000000242144. input_tokens=166, output_tokens=43
01:30:17,712 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,713 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8470000000088476. input_tokens=247, output_tokens=77
01:30:17,731 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,732 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0199999999604188. input_tokens=211, output_tokens=47
01:30:17,755 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9230000000097789. input_tokens=240, output_tokens=69
01:30:17,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2669999999925494. input_tokens=204, output_tokens=70
01:30:17,810 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9029999999911524. input_tokens=172, output_tokens=63
01:30:17,845 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1049999999813735. input_tokens=279, output_tokens=81
01:30:17,851 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0530000000144355. input_tokens=165, output_tokens=61
01:30:17,876 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.992000000027474. input_tokens=173, output_tokens=77
01:30:17,966 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6990000000223517. input_tokens=175, output_tokens=39
01:30:17,986 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:17,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0340000000433065. input_tokens=215, output_tokens=86
01:30:18,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0239999999757856. input_tokens=179, output_tokens=49
01:30:18,124 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,126 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8429999999934807. input_tokens=198, output_tokens=61
01:30:18,149 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8249999999534339. input_tokens=169, output_tokens=64
01:30:18,218 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1389999999664724. input_tokens=169, output_tokens=80
01:30:18,283 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.007999999972526. input_tokens=177, output_tokens=55
01:30:18,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9569999999948777. input_tokens=170, output_tokens=43
01:30:18,412 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8040000000037253. input_tokens=173, output_tokens=47
01:30:18,415 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7119999999995343. input_tokens=177, output_tokens=46
01:30:18,420 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0919999999459833. input_tokens=195, output_tokens=48
01:30:18,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.885999999998603. input_tokens=195, output_tokens=51
01:30:18,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9200000000419095. input_tokens=194, output_tokens=45
01:30:18,506 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8439999999827705. input_tokens=178, output_tokens=52
01:30:18,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9130000000004657. input_tokens=196, output_tokens=51
01:30:18,660 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3040000000037253. input_tokens=189, output_tokens=57
01:30:18,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:18,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3910000000032596. input_tokens=205, output_tokens=74
01:30:18,995 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
01:30:19,126 graphrag.index.run INFO Running workflow: create_base_entity_graph...
01:30:19,126 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
01:30:19,126 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
01:30:19,133 datashaper.workflow.workflow INFO executing verb cluster_graph
01:30:19,187 datashaper.workflow.workflow INFO executing verb select
01:30:19,189 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:30:19,266 graphrag.index.run INFO Running workflow: create_final_entities...
01:30:19,266 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:30:19,266 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
01:30:19,275 datashaper.workflow.workflow INFO executing verb unpack_graph
01:30:19,295 datashaper.workflow.workflow INFO executing verb rename
01:30:19,299 datashaper.workflow.workflow INFO executing verb select
01:30:19,303 datashaper.workflow.workflow INFO executing verb dedupe
01:30:19,308 datashaper.workflow.workflow INFO executing verb rename
01:30:19,312 datashaper.workflow.workflow INFO executing verb filter
01:30:19,324 datashaper.workflow.workflow INFO executing verb text_split
01:30:19,336 datashaper.workflow.workflow INFO executing verb drop
01:30:19,340 datashaper.workflow.workflow INFO executing verb merge
01:30:19,373 datashaper.workflow.workflow INFO executing verb text_embed
01:30:19,374 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
01:30:19,380 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
01:30:19,380 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
01:30:19,403 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 470 inputs via 470 snippets using 30 batches. max_batch_size=16, max_tokens=8191
01:30:19,666 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,691 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,693 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,703 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,712 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,723 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,749 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,774 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,781 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,806 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,814 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,831 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,842 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,913 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:19,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5580000000190921. input_tokens=518, output_tokens=0
01:30:19,981 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,23 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,71 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6620000000111759. input_tokens=861, output_tokens=0
01:30:20,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6890000000130385. input_tokens=529, output_tokens=0
01:30:20,287 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,517 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,585 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,653 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,709 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:20,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3099999999976717. input_tokens=573, output_tokens=0
01:30:20,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3250000000116415. input_tokens=1025, output_tokens=0
01:30:20,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3429999999934807. input_tokens=664, output_tokens=0
01:30:20,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3839999999618158. input_tokens=796, output_tokens=0
01:30:20,946 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9689999999827705. input_tokens=663, output_tokens=0
01:30:20,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5380000000004657. input_tokens=622, output_tokens=0
01:30:20,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5670000000391155. input_tokens=534, output_tokens=0
01:30:20,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5720000000437722. input_tokens=875, output_tokens=0
01:30:21,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6199999999953434. input_tokens=334, output_tokens=0
01:30:21,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6479999999864958. input_tokens=545, output_tokens=0
01:30:21,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.650000000023283. input_tokens=785, output_tokens=0
01:30:21,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6539999999804422. input_tokens=583, output_tokens=0
01:30:21,87 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:21,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6759999999776483. input_tokens=566, output_tokens=0
01:30:21,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.676000000035856. input_tokens=530, output_tokens=0
01:30:21,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6959999999962747. input_tokens=683, output_tokens=0
01:30:21,114 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
01:30:21,122 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.702999999979511. input_tokens=564, output_tokens=0
01:30:21,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7179999999934807. input_tokens=734, output_tokens=0
01:30:21,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7349999999860302. input_tokens=550, output_tokens=0
01:30:21,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7299999999813735. input_tokens=527, output_tokens=0
01:30:21,160 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7459999999846332. input_tokens=517, output_tokens=0
01:30:21,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.062000000034459. input_tokens=589, output_tokens=0
01:30:21,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7669999999925494. input_tokens=863, output_tokens=0
01:30:21,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4299999999930151. input_tokens=191, output_tokens=0
01:30:21,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4690000000409782. input_tokens=650, output_tokens=0
01:30:21,232 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8149999999441206. input_tokens=647, output_tokens=0
01:30:21,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8390000000363216. input_tokens=547, output_tokens=0
01:30:21,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1739999999990687. input_tokens=675, output_tokens=0
01:30:21,269 datashaper.workflow.workflow INFO executing verb drop
01:30:21,274 datashaper.workflow.workflow INFO executing verb filter
01:30:21,282 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:30:21,410 graphrag.index.run INFO Running workflow: create_final_nodes...
01:30:21,410 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:30:21,410 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
01:30:21,422 datashaper.workflow.workflow INFO executing verb layout_graph
01:30:21,496 datashaper.workflow.workflow INFO executing verb unpack_graph
01:30:21,519 datashaper.workflow.workflow INFO executing verb unpack_graph
01:30:21,633 datashaper.workflow.workflow INFO executing verb drop
01:30:21,640 datashaper.workflow.workflow INFO executing verb filter
01:30:21,674 datashaper.workflow.workflow INFO executing verb select
01:30:21,681 datashaper.workflow.workflow INFO executing verb rename
01:30:21,688 datashaper.workflow.workflow INFO executing verb join
01:30:21,704 datashaper.workflow.workflow INFO executing verb convert
01:30:21,731 datashaper.workflow.workflow INFO executing verb rename
01:30:21,733 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:30:21,831 graphrag.index.run INFO Running workflow: create_final_communities...
01:30:21,831 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:30:21,831 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
01:30:21,847 datashaper.workflow.workflow INFO executing verb unpack_graph
01:30:21,889 datashaper.workflow.workflow INFO executing verb unpack_graph
01:30:21,914 datashaper.workflow.workflow INFO executing verb aggregate_override
01:30:21,924 datashaper.workflow.workflow INFO executing verb join
01:30:21,934 datashaper.workflow.workflow INFO executing verb join
01:30:21,944 datashaper.workflow.workflow INFO executing verb concat
01:30:21,952 datashaper.workflow.workflow INFO executing verb filter
01:30:21,990 datashaper.workflow.workflow INFO executing verb aggregate_override
01:30:22,0 datashaper.workflow.workflow INFO executing verb join
01:30:22,11 datashaper.workflow.workflow INFO executing verb filter
01:30:22,31 datashaper.workflow.workflow INFO executing verb fill
01:30:22,39 datashaper.workflow.workflow INFO executing verb merge
01:30:22,53 datashaper.workflow.workflow INFO executing verb copy
01:30:22,70 datashaper.workflow.workflow INFO executing verb select
01:30:22,71 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:30:22,170 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
01:30:22,170 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
01:30:22,170 graphrag.index.run INFO read table from storage: create_final_entities.parquet
01:30:22,197 datashaper.workflow.workflow INFO executing verb select
01:30:22,205 datashaper.workflow.workflow INFO executing verb unroll
01:30:22,214 datashaper.workflow.workflow INFO executing verb aggregate_override
01:30:22,216 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
01:30:22,302 graphrag.index.run INFO Running workflow: create_final_relationships...
01:30:22,302 graphrag.index.run INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
01:30:22,303 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
01:30:22,305 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
01:30:22,323 datashaper.workflow.workflow INFO executing verb unpack_graph
01:30:22,348 datashaper.workflow.workflow INFO executing verb filter
01:30:22,370 datashaper.workflow.workflow INFO executing verb rename
01:30:22,379 datashaper.workflow.workflow INFO executing verb filter
01:30:22,400 datashaper.workflow.workflow INFO executing verb drop
01:30:22,411 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
01:30:22,422 datashaper.workflow.workflow INFO executing verb convert
01:30:22,440 datashaper.workflow.workflow INFO executing verb convert
01:30:22,442 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:30:22,534 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
01:30:22,534 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
01:30:22,534 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
01:30:22,553 datashaper.workflow.workflow INFO executing verb select
01:30:22,564 datashaper.workflow.workflow INFO executing verb unroll
01:30:22,578 datashaper.workflow.workflow INFO executing verb aggregate_override
01:30:22,591 datashaper.workflow.workflow INFO executing verb select
01:30:22,592 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
01:30:22,685 graphrag.index.run INFO Running workflow: create_final_community_reports...
01:30:22,688 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
01:30:22,688 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
01:30:22,690 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
01:30:22,712 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
01:30:22,726 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
01:30:22,737 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
01:30:22,750 datashaper.workflow.workflow INFO executing verb prepare_community_reports
01:30:22,750 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 470
01:30:22,774 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 470
01:30:22,807 datashaper.workflow.workflow INFO executing verb create_community_reports
01:30:27,246 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:27,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.431999999971595. input_tokens=2197, output_tokens=386
01:30:27,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:27,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.684000000008382. input_tokens=2133, output_tokens=462
01:30:27,620 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:27,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.779999999969732. input_tokens=2198, output_tokens=430
01:30:27,648 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:27,654 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.830000000016298. input_tokens=2534, output_tokens=520
01:30:27,861 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:27,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.032999999995809. input_tokens=2461, output_tokens=578
01:30:28,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:28,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.5939999999827705. input_tokens=2224, output_tokens=599
01:30:28,681 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:28,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.866000000038184. input_tokens=2755, output_tokens=576
01:30:30,110 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:30,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.255999999993946. input_tokens=3026, output_tokens=754
01:30:30,564 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:30,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.717999999993481. input_tokens=2907, output_tokens=795
01:30:30,831 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:30,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.006999999983236. input_tokens=2637, output_tokens=639
01:30:31,33 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:31,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.190000000002328. input_tokens=2417, output_tokens=722
01:30:31,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:31,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.475999999966007. input_tokens=2896, output_tokens=698
01:30:32,611 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:32,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.804999999993015. input_tokens=2831, output_tokens=800
01:30:35,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:35,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.693000000028405. input_tokens=2501, output_tokens=871
01:30:40,915 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:40,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.341000000014901. input_tokens=2594, output_tokens=548
01:30:41,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:41,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.010999999998603. input_tokens=2912, output_tokens=556
01:30:42,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:42,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.304000000003725. input_tokens=2674, output_tokens=685
01:30:43,773 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:43,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.184999999997672. input_tokens=2784, output_tokens=747
01:30:43,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:43,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.223999999987427. input_tokens=4516, output_tokens=794
01:30:43,820 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:43,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.227000000013504. input_tokens=3143, output_tokens=812
01:30:43,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:43,998 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.398999999975786. input_tokens=3623, output_tokens=768
01:30:44,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:44,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.493000000016764. input_tokens=3553, output_tokens=754
01:30:48,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
01:30:48,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.683000000019092. input_tokens=3515, output_tokens=801
01:30:48,297 datashaper.workflow.workflow INFO executing verb window
01:30:48,300 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:30:48,471 graphrag.index.run INFO Running workflow: create_final_text_units...
01:30:48,475 graphrag.index.run INFO dependencies for create_final_text_units: ['join_text_units_to_relationship_ids', 'create_base_text_units', 'join_text_units_to_entity_ids']
01:30:48,475 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
01:30:48,479 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
01:30:48,483 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
01:30:48,505 datashaper.workflow.workflow INFO executing verb select
01:30:48,516 datashaper.workflow.workflow INFO executing verb rename
01:30:48,529 datashaper.workflow.workflow INFO executing verb join
01:30:48,543 datashaper.workflow.workflow INFO executing verb join
01:30:48,558 datashaper.workflow.workflow INFO executing verb aggregate_override
01:30:48,573 datashaper.workflow.workflow INFO executing verb select
01:30:48,575 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:30:48,672 graphrag.index.run INFO Running workflow: create_base_documents...
01:30:48,672 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
01:30:48,672 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
01:30:48,695 datashaper.workflow.workflow INFO executing verb unroll
01:30:48,708 datashaper.workflow.workflow INFO executing verb select
01:30:48,719 datashaper.workflow.workflow INFO executing verb rename
01:30:48,730 datashaper.workflow.workflow INFO executing verb join
01:30:48,744 datashaper.workflow.workflow INFO executing verb aggregate_override
01:30:48,756 datashaper.workflow.workflow INFO executing verb join
01:30:48,771 datashaper.workflow.workflow INFO executing verb rename
01:30:48,783 datashaper.workflow.workflow INFO executing verb convert
01:30:48,809 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
01:30:48,913 graphrag.index.run INFO Running workflow: create_final_documents...
01:30:48,914 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
01:30:48,914 graphrag.index.run INFO read table from storage: create_base_documents.parquet
01:30:48,939 datashaper.workflow.workflow INFO executing verb rename
01:30:48,940 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
